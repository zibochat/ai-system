#!/usr/bin/env python3
"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø±Ø¹Øª Ùˆ Ø¯Ù‚Øª ZiboChat
"""

import json
import time
import asyncio
from pathlib import Path
from typing import Dict, List, Any
import logging

# Import modules
from recommender import load_data, index_products_to_faiss, recommend
from llm_agent import upsert_user_docs, retrieve_user_memory

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PerformanceOptimizer:
    def __init__(self):
        self.data_dir = Path("data")
        self.faiss_dir = self.data_dir / "faiss"
        self.faiss_dir.mkdir(exist_ok=True)
        
    def preload_data(self):
        """Ù¾ÛŒØ´â€ŒØ¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
        logger.info("ğŸ”„ Ù¾ÛŒØ´â€ŒØ¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
        
        start_time = time.time()
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª Ùˆ Ù†Ø¸Ø±Ø§Øª
        products, comments = load_data()
        
        load_time = time.time() - start_time
        logger.info(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯ ({load_time:.2f}s)")
        logger.info(f"ğŸ“Š {len(products)} Ù…Ø­ØµÙˆÙ„ Ùˆ {len(comments)} Ù†Ø¸Ø±")
        
        return products, comments
    
    def create_product_summaries(self, products: List[Dict], comments: List[Dict]):
        """Ø§ÛŒØ¬Ø§Ø¯ Ø®Ù„Ø§ØµÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ±"""
        logger.info("ğŸ”„ Ø§ÛŒØ¬Ø§Ø¯ Ø®Ù„Ø§ØµÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª...")
        
        summaries = {}
        start_time = time.time()
        
        for product in products:
            product_id = str(product.get("id"))
            name = product.get("full_name") or product.get("nameFa") or product.get("nameEn") or ""
            desc = product.get("description", "")
            
            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù†Ø¸Ø±Ø§Øª Ù…Ø±ØªØ¨Ø·
            related_comments = [c for c in comments if c.get("product_id") == product_id]
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ø®Ù„Ø§ØµÙ‡ Ø³Ø±ÛŒØ¹
            if related_comments:
                comment_texts = [c.get("description", "")[:200] for c in related_comments[:5]]
                summary = f"{name}\n{desc[:300]}\nÙ†Ø¸Ø±Ø§Øª: {' '.join(comment_texts)}"
            else:
                summary = f"{name}\n{desc[:500]}"
            
            summaries[product_id] = summary
        
        summary_time = time.time() - start_time
        logger.info(f"âœ… Ø®Ù„Ø§ØµÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù†Ø¯ ({summary_time:.2f}s)")
        
        return summaries
    
    def optimize_faiss_index(self, products: List[Dict], comments: List[Dict]):
        """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ FAISS index"""
        logger.info("ğŸ”„ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ FAISS index...")
        
        start_time = time.time()
        
        try:
            # Ø§ÛŒØ¬Ø§Ø¯ index Ù…Ø­ØµÙˆÙ„Ø§Øª
            count = index_products_to_faiss("products_index")
            
            index_time = time.time() - start_time
            logger.info(f"âœ… FAISS index Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯ ({index_time:.2f}s)")
            logger.info(f"ğŸ“Š {count} Ù…Ø­ØµÙˆÙ„ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø´Ø¯")
            
            return True
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ FAISS: {e}")
            return False
    
    def create_user_profiles_cache(self):
        """Ø§ÛŒØ¬Ø§Ø¯ cache Ù¾Ø±ÙˆÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±"""
        logger.info("ğŸ”„ Ø§ÛŒØ¬Ø§Ø¯ cache Ù¾Ø±ÙˆÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±...")
        
        try:
            from recommender import load_all_profiles
            profiles = load_all_profiles(limit=100)  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ø±Ø§ÛŒ ØªØ³Øª
            
            start_time = time.time()
            
            for i, profile in enumerate(profiles):
                user_id = f"profile_{i}"
                profile_text = json.dumps(profile, ensure_ascii=False)
                
                # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± FAISS
                doc = {
                    "id": f"profile_{user_id}",
                    "text": profile_text,
                    "meta": {"type": "profile", "user_id": user_id}
                }
                upsert_user_docs(user_id, [doc])
            
            cache_time = time.time() - start_time
            logger.info(f"âœ… Cache Ù¾Ø±ÙˆÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯ ({cache_time:.2f}s)")
            logger.info(f"ğŸ“Š {len(profiles)} Ù¾Ø±ÙˆÙØ§ÛŒÙ„ cache Ø´Ø¯")
            
            return True
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ cache Ù¾Ø±ÙˆÙØ§ÛŒÙ„â€ŒÙ‡Ø§: {e}")
            return False
    
    def test_recommendation_speed(self, products: List[Dict], comments: List[Dict]):
        """ØªØ³Øª Ø³Ø±Ø¹Øª ØªÙˆØµÛŒÙ‡"""
        logger.info("ğŸ”„ ØªØ³Øª Ø³Ø±Ø¹Øª ØªÙˆØµÛŒÙ‡...")
        
        test_cases = [
            "Ø³Ù„Ø§Ù…",
            "Ú†Ù‡ Ù…Ø­ØµÙˆÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÙˆØ³Øª Ø®Ø´Ú© Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯ØŸ",
            "Ú©Ø±Ù… Ù…Ø±Ø·ÙˆØ¨ Ú©Ù†Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ù…",
            "Ø¶Ø¯ Ø¢ÙØªØ§Ø¨ Ø¨Ø±Ø§ÛŒ Ù¾ÙˆØ³Øª Ø­Ø³Ø§Ø³",
            "Ù…Ø§Ø³Ú© ØµÙˆØ±Øª"
        ]
        
        profile = {"skin_type": "Ø®Ø´Ú©", "age": 25}
        
        times = []
        for test_case in test_cases:
            start_time = time.time()
            try:
                answer, log = recommend(profile, test_case, max_count=3, user_id="test_user")
                end_time = time.time()
                response_time = end_time - start_time
                times.append(response_time)
                logger.info(f"âœ… '{test_case[:30]}...' - {response_time:.2f}s")
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ³Øª '{test_case}': {e}")
                times.append(float('inf'))
        
        avg_time = sum(times) / len(times) if times else 0
        logger.info(f"ğŸ“Š Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø®: {avg_time:.2f}s")
        
        return avg_time
    
    def optimize_memory_usage(self):
        """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø­Ø§ÙØ¸Ù‡"""
        logger.info("ğŸ”„ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø­Ø§ÙØ¸Ù‡...")
        
        try:
            # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† cache Ù‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ
            import gc
            gc.collect()
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø­Ø§ÙØ¸Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡
            import psutil
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            
            logger.info(f"ğŸ“Š Ø­Ø§ÙØ¸Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡: {memory_mb:.2f} MB")
            
            return memory_mb
        except ImportError:
            logger.warning("âš ï¸ psutil Ù†ØµØ¨ Ù†Ø´Ø¯Ù‡ - Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø­Ø§ÙØ¸Ù‡ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø±Ø¯")
            return 0
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡: {e}")
            return 0
    
    def create_performance_report(self, results: Dict[str, Any]):
        """Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯"""
        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "optimization_results": results,
            "recommendations": []
        }
        
        # ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
        if results.get("avg_response_time", 0) > 5:
            report["recommendations"].append("Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø® Ø¨Ø§Ù„Ø§ Ø§Ø³Øª - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² cache Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯")
        
        if results.get("memory_usage", 0) > 1000:
            report["recommendations"].append("Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø­Ø§ÙØ¸Ù‡ Ø¨Ø§Ù„Ø§ Ø§Ø³Øª - Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯")
        
        if not results.get("faiss_optimized", False):
            report["recommendations"].append("FAISS index Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ø´Ø¯Ù‡ - Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
        report_file = Path("performance_report.json")
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“„ Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø± {report_file} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        return report
    
    def run_optimization(self):
        """Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„"""
        logger.info("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ZiboChat...")
        logger.info("=" * 50)
        
        results = {}
        
        # Ù¾ÛŒØ´â€ŒØ¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        products, comments = self.preload_data()
        results["data_loaded"] = True
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø®Ù„Ø§ØµÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª
        summaries = self.create_product_summaries(products, comments)
        results["summaries_created"] = len(summaries)
        
        # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ FAISS
        faiss_success = self.optimize_faiss_index(products, comments)
        results["faiss_optimized"] = faiss_success
        
        # Ø§ÛŒØ¬Ø§Ø¯ cache Ù¾Ø±ÙˆÙØ§ÛŒÙ„â€ŒÙ‡Ø§
        profile_cache_success = self.create_user_profiles_cache()
        results["profile_cache_created"] = profile_cache_success
        
        # ØªØ³Øª Ø³Ø±Ø¹Øª
        avg_time = self.test_recommendation_speed(products, comments)
        results["avg_response_time"] = avg_time
        
        # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡
        memory_usage = self.optimize_memory_usage()
        results["memory_usage"] = memory_usage
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´
        report = self.create_performance_report(results)
        
        logger.info("\n" + "=" * 50)
        logger.info("ğŸ‰ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯!")
        logger.info(f"ğŸ“Š Ù†ØªØ§ÛŒØ¬: {results}")
        
        return results

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    optimizer = PerformanceOptimizer()
    optimizer.run_optimization()

if __name__ == "__main__":
    main()

